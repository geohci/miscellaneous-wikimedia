{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language-Agnostic Quality Model\n",
    "Building off a couple of papers:\n",
    "* https://grouplens.org/site-content/uploads/2013/09/wikisym2013_warnckewang-cosley-riedl.pdf\n",
    "* http://lewoniewski.info/files/bis2017_measure.pdf and https://www.mdpi.com/2227-9709/4/4/43/htm#B21-informatics-04-00043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / settings / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import mwparserfromhell\n",
    "import wmfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/isaacj/.conda/envs/2021-03-18T15.28.24_isaacj/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "spark = wmfdata.spark.get_session(app_name='pyspark large; quality-model-features',\n",
    "                                  type='yarn-large', # local, yarn-regular, yarn-large\n",
    "                                  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition.mode = nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikidata:\n",
      "+-------------------+\n",
      "|partition          |\n",
      "+-------------------+\n",
      "|snapshot=2021-03-22|\n",
      "|snapshot=2021-03-29|\n",
      "|snapshot=2021-04-05|\n",
      "|snapshot=2021-04-12|\n",
      "|snapshot=2021-04-26|\n",
      "|snapshot=2021-05-03|\n",
      "|snapshot=2021-05-10|\n",
      "+-------------------+\n",
      "\n",
      "\n",
      "Mediawiki:\n",
      "+------------------------+\n",
      "|partition               |\n",
      "+------------------------+\n",
      "|snapshot=2016-12_private|\n",
      "|snapshot=2017-07_private|\n",
      "|snapshot=2020-11        |\n",
      "|snapshot=2020-12        |\n",
      "|snapshot=2021-01        |\n",
      "|snapshot=2021-02        |\n",
      "|snapshot=2021-03        |\n",
      "|snapshot=2021-04        |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Wikidata:\")\n",
    "spark.sql('show partitions wmf.wikidata_item_page_link').show(50, False)\n",
    "print(\"\\nMediawiki:\")\n",
    "spark.sql(\"show partitions wmf_raw.mediawiki_project_namespace_map\").show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = '2021-04'  # data will be current to this date -- e.g., 2020-07 means data is up to 30 June 2020 (at least)\n",
    "wd_snapshot = '2021-04-05'\n",
    "year = 2021\n",
    "last_month = 4\n",
    "allwikis_tablename = 'isaacj.qual_features'\n",
    "qual_preds_tablename = 'isaacj.qual_preds'\n",
    "norm_pvs_tablename = 'isaacj.normed_pvs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getNumRefs(wikitext)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfn_templates = [t.lower() for t in [\"Shortened footnote template\", \"sfn\", \"Sfnp\", \"Sfnm\", \"Sfnmp\"]]\n",
    "# NOTE: don't include citation templates like Cite or Harv because they are (or should be) wrapped in ref tags\n",
    "\n",
    "def getNumRefs(wikitext):\n",
    "    \"\"\"Extract list of links from wikitext for an article via mwparserfromhell.\"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        num_ref_tags = len([t.tag for t in wt.filter_tags() if t.tag == 'ref'])\n",
    "        num_sftn_templates = len([t.name for t in wt.filter_templates() if t.name.lower() in sfn_templates])\n",
    "        return num_ref_tags + num_sftn_templates\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getNumRefs', getNumRefs, 'Int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getNumHeadings(wikitext, max_level=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumHeadings(wikitext, max_level=None):\n",
    "    \"\"\"Extract list of headings from wikitext for an article.\"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        if max_level is None:\n",
    "            return len([1 for l in wt.filter_headings()])\n",
    "        else:\n",
    "            return len([1 for l in wt.filter_headings() if l.level <= max_level])\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getNumHeadings', getNumHeadings, 'Int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_execute = True\n",
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {allwikis_tablename} (\n",
    "        page_id          INT     COMMENT 'Article page ID',\n",
    "        page_len         INT     COMMENT 'Number of bytes in article wikitext',\n",
    "        num_images       INT     COMMENT 'Number of unique images in article',\n",
    "        num_refs         INT     COMMENT 'Number of references in article',\n",
    "        num_headings     INT     COMMENT 'Number of Level-2 and Level-3 headings in article'\n",
    "    )\n",
    "    PARTITIONED BY (\n",
    "        wiki_db          STRING  COMMENT 'Wiki dbname -- e.g., enwiki for English Wikipedia'\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "if do_execute:\n",
    "    spark.sql(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate feature data for all wikis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH wikipedia_projects AS (\n",
      "    SELECT DISTINCT\n",
      "      dbname\n",
      "    FROM wmf_raw.mediawiki_project_namespace_map\n",
      "    WHERE\n",
      "      snapshot = '2021-04'\n",
      "      AND hostname LIKE '%wikipedia%'\n",
      "      AND dbname <> 'enwiki'\n",
      "),\n",
      "pages AS (\n",
      "    SELECT\n",
      "      wiki_db,\n",
      "      page_id,\n",
      "      COALESCE(page_len, 0) AS page_len\n",
      "    FROM wmf_raw.mediawiki_page p\n",
      "    INNER JOIN wikipedia_projects wp\n",
      "      ON (p.wiki_db = wp.dbname)\n",
      "    WHERE\n",
      "      snapshot = '2021-04'\n",
      "      AND page_namespace = 0\n",
      "      AND NOT page_is_redirect\n",
      "),\n",
      "num_images AS (\n",
      "    SELECT\n",
      "      i.wiki_db,\n",
      "      il_from AS page_id,\n",
      "      COUNT(DISTINCT(il_to)) AS num_images\n",
      "    FROM wmf_raw.mediawiki_imagelinks i\n",
      "    INNER JOIN pages p\n",
      "      ON (i.il_from = p.page_id\n",
      "          AND i.wiki_db = p.wiki_db)\n",
      "    WHERE\n",
      "      snapshot = '2021-04'\n",
      "      AND il_from_namespace = 0\n",
      "    GROUP BY\n",
      "      i.wiki_db,\n",
      "      il_from\n",
      "),\n",
      "wikitext_stats AS (\n",
      "    SELECT\n",
      "      wt.wiki_db,\n",
      "      wt.page_id,\n",
      "      getNumRefs(revision_text) AS num_refs,\n",
      "      getNumHeadings(revision_text, 3) AS num_headings\n",
      "    FROM wmf.mediawiki_wikitext_current wt\n",
      "    INNER JOIN pages p\n",
      "      ON (wt.page_id = p.page_id\n",
      "          AND wt.wiki_db = p.wiki_db)\n",
      "    WHERE\n",
      "      snapshot = '2021-04'\n",
      "      AND page_namespace = 0\n",
      ")\n",
      "INSERT OVERWRITE TABLE isaacj.qual_features\n",
      "SELECT\n",
      "  p.page_id,\n",
      "  page_len,\n",
      "  COALESCE(num_images, 0) AS num_images,\n",
      "  COALESCE(num_refs, 0) AS num_refs,\n",
      "  COALESCE(num_headings, 0) AS num_headings,\n",
      "  p.wiki_db AS wiki_db\n",
      "FROM pages p\n",
      "LEFT JOIN num_images i\n",
      "  ON (p.wiki_db = i.wiki_db\n",
      "      AND p.page_id = i.page_id)\n",
      "LEFT JOIN wikitext_stats wt\n",
      "  ON (p.wiki_db = wt.wiki_db\n",
      "      AND p.page_id = wt.page_id)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALL WIKIS!!\n",
    "\n",
    "The quality model requires the following attributes:\n",
    "* # bytes (page length)\n",
    "* # of references\n",
    "* # of images\n",
    "* # of headers (just levels 2 + 3)\n",
    "\n",
    "Explanation of CTEs:\n",
    "* \n",
    "\"\"\"\n",
    "\n",
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH wikipedia_projects AS (\n",
    "    SELECT DISTINCT\n",
    "      dbname\n",
    "    FROM wmf_raw.mediawiki_project_namespace_map\n",
    "    WHERE\n",
    "      snapshot = '{snapshot}'\n",
    "      AND hostname LIKE '%wikipedia%'\n",
    "),\n",
    "pages AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      page_id,\n",
    "      COALESCE(page_len, 0) AS page_len\n",
    "    FROM wmf_raw.mediawiki_page p\n",
    "    INNER JOIN wikipedia_projects wp\n",
    "      ON (p.wiki_db = wp.dbname)\n",
    "    WHERE\n",
    "      snapshot = '{snapshot}'\n",
    "      AND page_namespace = 0\n",
    "      AND NOT page_is_redirect\n",
    "),\n",
    "num_images AS (\n",
    "    SELECT\n",
    "      i.wiki_db,\n",
    "      il_from AS page_id,\n",
    "      COUNT(DISTINCT(il_to)) AS num_images\n",
    "    FROM wmf_raw.mediawiki_imagelinks i\n",
    "    INNER JOIN pages p\n",
    "      ON (i.il_from = p.page_id\n",
    "          AND i.wiki_db = p.wiki_db)\n",
    "    WHERE\n",
    "      snapshot = '{snapshot}'\n",
    "      AND il_from_namespace = 0\n",
    "    GROUP BY\n",
    "      i.wiki_db,\n",
    "      il_from\n",
    "),\n",
    "wikitext_stats AS (\n",
    "    SELECT\n",
    "      wt.wiki_db,\n",
    "      wt.page_id,\n",
    "      getNumRefs(revision_text) AS num_refs,\n",
    "      getNumHeadings(revision_text, 3) AS num_headings\n",
    "    FROM wmf.mediawiki_wikitext_current wt\n",
    "    INNER JOIN pages p\n",
    "      ON (wt.page_id = p.page_id\n",
    "          AND wt.wiki_db = p.wiki_db)\n",
    "    WHERE\n",
    "      snapshot = '{snapshot}'\n",
    "      AND page_namespace = 0\n",
    ")\n",
    "INSERT OVERWRITE TABLE {allwikis_tablename}\n",
    "SELECT\n",
    "  p.page_id,\n",
    "  page_len,\n",
    "  COALESCE(num_images, 0) AS num_images,\n",
    "  COALESCE(num_refs, 0) AS num_refs,\n",
    "  COALESCE(num_headings, 0) AS num_headings,\n",
    "  p.wiki_db AS wiki_db\n",
    "FROM pages p\n",
    "LEFT JOIN num_images i\n",
    "  ON (p.wiki_db = i.wiki_db\n",
    "      AND p.page_id = i.page_id)\n",
    "LEFT JOIN wikitext_stats wt\n",
    "  ON (p.wiki_db = wt.wiki_db\n",
    "      AND p.page_id = wt.page_id)\n",
    "\"\"\"\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misalignment for All Wikis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Quality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.predictQuality(loglength, num_images, num_headings, num_refs)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictQuality(loglength, num_images, num_headings, num_refs):\n",
    "    \"\"\"Predict quality of article.\"\"\"\n",
    "        \n",
    "    coef_len = 0.258\n",
    "    coef_img = 0.015\n",
    "    coef_hea = 0.241\n",
    "    coef_ref = 0.486\n",
    "    try:\n",
    "        pred = (coef_len * loglength) + (coef_img * num_images) + (coef_hea * num_headings) + (coef_ref * num_refs)\n",
    "        return pred\n",
    "    except Exception:\n",
    "        return None\n",
    "        \n",
    "spark.udf.register('predictQuality', predictQuality, 'Float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_execute = True\n",
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {qual_preds_tablename} (\n",
    "        page_id          INT     COMMENT 'Article page ID',\n",
    "        item_id          STRING  COMMENT 'Associated Wikidata QID',\n",
    "        pred_qual        FLOAT   COMMENT 'Predicted quality score [0-1]; 0 = no content; 1 = highest quality'\n",
    "    )\n",
    "    PARTITIONED BY (\n",
    "        wiki_db          STRING  COMMENT 'Wiki dbname -- e.g., enwiki for English Wikipedia'\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "if do_execute:\n",
    "    spark.sql(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH wikipedia_projects AS (\n",
      "    SELECT DISTINCT\n",
      "      dbname,\n",
      "      SUBSTR(hostname, 1, LENGTH(hostname)-4) AS project\n",
      "    FROM wmf_raw.mediawiki_project_namespace_map\n",
      "    WHERE\n",
      "      snapshot = '2021-04'\n",
      "      AND hostname LIKE '%wikipedia%'\n",
      "),\n",
      "all_pages AS (\n",
      "    SELECT\n",
      "      wiki_db,\n",
      "      page_id\n",
      "    FROM wmf_raw.mediawiki_page p\n",
      "    INNER JOIN wikipedia_projects wp\n",
      "      ON (p.wiki_db = wp.dbname)\n",
      "    WHERE\n",
      "      snapshot = '2021-04'\n",
      "      AND page_namespace = 0\n",
      "      AND NOT page_is_redirect\n",
      "),\n",
      "wikidata_ids AS (\n",
      "    SELECT\n",
      "      wiki_db,\n",
      "      page_id,\n",
      "      item_id\n",
      "    FROM wmf.wikidata_item_page_link wd\n",
      "    INNER JOIN wikipedia_projects wp\n",
      "      ON (wd.wiki_db = wp.dbname)\n",
      "    WHERE\n",
      "      snapshot = '2021-04-05'\n",
      "      AND page_namespace = 0\n",
      "),\n",
      "max_vals AS (\n",
      "    SELECT\n",
      "      wiki_db,\n",
      "      PERCENTILE_APPROX(LOG10(1 + page_len), 0.95) AS max_length,\n",
      "      LEAST(5, PERCENTILE(num_images, 0.95)) AS max_images,\n",
      "      LEAST(5, PERCENTILE(num_headings, 0.95)) AS max_headings,\n",
      "      LEAST(10, PERCENTILE(num_refs, 0.95)) AS max_refs\n",
      "    FROM isaacj.qual_features qf\n",
      "    INNER JOIN wikipedia_projects wp\n",
      "      ON (qf.wiki_db = wp.dbname)\n",
      "    WHERE\n",
      "      page_len IS NOT NULL\n",
      "      AND num_images IS NOT NULL\n",
      "      AND num_headings IS NOT NULL\n",
      "      AND num_refs IS NOT NULL\n",
      "    GROUP BY\n",
      "      wiki_db\n",
      "),\n",
      "qual_features_trimmed AS (\n",
      "    SELECT\n",
      "      page_id,\n",
      "      qf.wiki_db,\n",
      "      COALESCE(LEAST(LOG10(1 + page_len), max_length), 0) / max_length AS len_x,\n",
      "      COALESCE(LEAST(num_images, max_images), 0) / max_images AS images_x,\n",
      "      COALESCE(LEAST(num_headings, max_headings), 0) / max_headings AS headings_x,\n",
      "      COALESCE(LEAST(num_refs, max_refs), 0) / max_refs AS refs_x\n",
      "    FROM isaacj.qual_features qf\n",
      "    INNER JOIN max_vals mv\n",
      "      ON (qf.wiki_db = mv.wiki_db)\n",
      "),\n",
      "qual_predictions AS (\n",
      "    SELECT\n",
      "      wiki_db,\n",
      "      page_id,\n",
      "      COALESCE(predictQuality(len_x, images_x, headings_x, refs_x), 0) AS pred_qual\n",
      "    FROM qual_features_trimmed\n",
      ")\n",
      "INSERT OVERWRITE TABLE isaacj.qual_preds\n",
      "SELECT\n",
      "  ap.page_id,\n",
      "  wd.item_id,\n",
      "  COALESCE(pred_qual, 0),\n",
      "  ap.wiki_db\n",
      "FROM all_pages ap\n",
      "LEFT JOIN qual_predictions qp\n",
      "  ON (ap.wiki_db = qp.wiki_db\n",
      "      AND ap.page_id = qp.page_id)\n",
      "LEFT JOIN wikidata_ids wd\n",
      "  ON (ap.wiki_db = wd.wiki_db\n",
      "      AND ap.page_id = wd.page_id)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "qual_pctile = 0.95\n",
    "MIN_IMAGES = 5\n",
    "MIN_HEADINGS = 5\n",
    "MIN_REFS = 10\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH wikipedia_projects AS (\n",
    "    SELECT DISTINCT\n",
    "      dbname,\n",
    "      SUBSTR(hostname, 1, LENGTH(hostname)-4) AS project\n",
    "    FROM wmf_raw.mediawiki_project_namespace_map\n",
    "    WHERE\n",
    "      snapshot = '{snapshot}'\n",
    "      AND hostname LIKE '%wikipedia%'\n",
    "),\n",
    "all_pages AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      page_id\n",
    "    FROM wmf_raw.mediawiki_page p\n",
    "    INNER JOIN wikipedia_projects wp\n",
    "      ON (p.wiki_db = wp.dbname)\n",
    "    WHERE\n",
    "      snapshot = '{snapshot}'\n",
    "      AND page_namespace = 0\n",
    "      AND NOT page_is_redirect\n",
    "),\n",
    "wikidata_ids AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      page_id,\n",
    "      item_id\n",
    "    FROM wmf.wikidata_item_page_link wd\n",
    "    INNER JOIN wikipedia_projects wp\n",
    "      ON (wd.wiki_db = wp.dbname)\n",
    "    WHERE\n",
    "      snapshot = '{wd_snapshot}'\n",
    "      AND page_namespace = 0\n",
    "),\n",
    "max_vals AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      PERCENTILE_APPROX(LOG10(1 + page_len), {qual_pctile}) AS max_length,\n",
    "      LEAST({MIN_IMAGES}, PERCENTILE(num_images, {qual_pctile})) AS max_images,\n",
    "      LEAST({MIN_HEADINGS}, PERCENTILE(num_headings, {qual_pctile})) AS max_headings,\n",
    "      LEAST({MIN_REFS}, PERCENTILE(num_refs, {qual_pctile})) AS max_refs\n",
    "    FROM {allwikis_tablename} qf\n",
    "    INNER JOIN wikipedia_projects wp\n",
    "      ON (qf.wiki_db = wp.dbname)\n",
    "    WHERE\n",
    "      page_len IS NOT NULL\n",
    "      AND num_images IS NOT NULL\n",
    "      AND num_headings IS NOT NULL\n",
    "      AND num_refs IS NOT NULL\n",
    "    GROUP BY\n",
    "      wiki_db\n",
    "),\n",
    "qual_features_trimmed AS (\n",
    "    SELECT\n",
    "      page_id,\n",
    "      qf.wiki_db,\n",
    "      COALESCE(LEAST(LOG10(1 + page_len), max_length), 0) / max_length AS len_x,\n",
    "      COALESCE(LEAST(num_images, max_images), 0) / max_images AS images_x,\n",
    "      COALESCE(LEAST(num_headings, max_headings), 0) / max_headings AS headings_x,\n",
    "      COALESCE(LEAST(num_refs, max_refs), 0) / max_refs AS refs_x\n",
    "    FROM {allwikis_tablename} qf\n",
    "    INNER JOIN max_vals mv\n",
    "      ON (qf.wiki_db = mv.wiki_db)\n",
    "),\n",
    "qual_predictions AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      page_id,\n",
    "      COALESCE(predictQuality(len_x, images_x, headings_x, refs_x), 0) AS pred_qual\n",
    "    FROM qual_features_trimmed\n",
    ")\n",
    "INSERT OVERWRITE TABLE {qual_preds_tablename}\n",
    "SELECT\n",
    "  ap.page_id,\n",
    "  wd.item_id,\n",
    "  COALESCE(pred_qual, 0),\n",
    "  ap.wiki_db\n",
    "FROM all_pages ap\n",
    "LEFT JOIN qual_predictions qp\n",
    "  ON (ap.wiki_db = qp.wiki_db\n",
    "      AND ap.page_id = qp.page_id)\n",
    "LEFT JOIN wikidata_ids wd\n",
    "  ON (ap.wiki_db = wd.wiki_db\n",
    "      AND ap.page_id = wd.page_id)\n",
    "\"\"\"\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)\n",
    "    #result.write.csv(path=\"/user/isaacj/quality-preds-allwikis\", compression='gzip', header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
