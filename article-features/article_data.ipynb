{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Data Features\n",
    "Example code snippets for calculating various standard features related to Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import mwparserfromhell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wmfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "spark = wmfdata.spark.get_session(app_name='pyspark reg; article-data-exploration',\n",
    "                                  type='yarn-regular', # local, yarn-regular, yarn-large\n",
    "                                  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mediawiki snapshot partitions:\n",
      "+------------------------+\n",
      "|partition               |\n",
      "+------------------------+\n",
      "|snapshot=2016-12_private|\n",
      "|snapshot=2017-07_private|\n",
      "|snapshot=2021-04        |\n",
      "|snapshot=2021-05        |\n",
      "|snapshot=2021-06        |\n",
      "|snapshot=2021-07        |\n",
      "|snapshot=2021-08        |\n",
      "|snapshot=2021-09        |\n",
      "+------------------------+\n",
      "\n",
      "\n",
      "Current wikidata snapshot partitions:\n",
      "+-------------------+\n",
      "|partition          |\n",
      "+-------------------+\n",
      "|snapshot=2021-08-30|\n",
      "|snapshot=2021-09-06|\n",
      "|snapshot=2021-09-13|\n",
      "|snapshot=2021-09-20|\n",
      "|snapshot=2021-09-27|\n",
      "|snapshot=2021-10-04|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Current mediawiki snapshot partitions:\")\n",
    "spark.sql(\"SHOW PARTITIONS wmf_raw.mediawiki_project_namespace_map\").show(50, False)\n",
    "\n",
    "print(\"\\nCurrent wikidata snapshot partitions:\")\n",
    "spark.sql(\"SHOW PARTITIONS wmf.wikidata_item_page_link\").show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediawiki_snapshot = '2021-09'  # data will be current to this month inclusive -- e.g., 2020-07 means data is up to 31 July 2020 (at least)\n",
    "wikidata_snapshot = '2021-10-04'  # data will be current to approximately this day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "Various random utility functions that can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNamespacePrefixes(lang):\n",
    "    \"\"\"Get list of namespace prefixes for a given wiki.\n",
    "    \n",
    "    This should be run in a standard Python cell and then the output\n",
    "    fed into a call to filterLinksByNs as part of a SQL query.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    base_url = \"https://{0}.wikipedia.org/w/api.php\".format(lang)\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"meta\": \"siteinfo\",\n",
    "        \"siprop\": \"namespacealiases|namespaces\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\"\n",
    "    }\n",
    "    result = session.get(url=base_url, params=params)\n",
    "    result = result.json()\n",
    "    prefix_to_ns = {}\n",
    "    if 'namespacealiases' in result.get('query', {}):\n",
    "        for alias in result['query']['namespacealiases']:\n",
    "            prefix_to_ns[alias['alias']] = alias['id']\n",
    "    if 'namespaces' in result.get('query', {}):\n",
    "        for ns in result['query']['namespaces'].values():\n",
    "            if 'name' in ns:\n",
    "                prefix_to_ns[ns['name'].replace(' ', '_')] = ns['id']\n",
    "            if 'canonical' in ns:\n",
    "                prefix_to_ns[ns['canonical'].replace(' ', '_')] = ns['id']\n",
    "    return prefix_to_ns\n",
    "\n",
    "def filterLinksByNs(links, keep_ns):\n",
    "    \"\"\"Filter down a list of link targets (page titles) by namespace -- e.g., keep_ns=6 for just File links.\"\"\"\n",
    "    for i in range(len(links)-1, -1, -1):\n",
    "        link_ns = 0\n",
    "        if ':' in links[i]:\n",
    "            prefix = links[i].split(':')[0].replace(' ', '_')\n",
    "            if prefix in namespace_prefixes:\n",
    "                link_ns = namespace_prefixes[prefix]\n",
    "        if link_ns not in keep_ns:\n",
    "            links.pop(i)\n",
    "    return links\n",
    "\n",
    "spark.udf.register('filterLinksByNs', filterLinksByNs, 'Array<String>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeComments(wikitext):\n",
    "    \"\"\"Wikitext can have comments in it that should be removed before checking for links etc.\n",
    "    \n",
    "    In practice this has no effect on pretty much every article but some articles do have large comments\n",
    "    in them with either context about the content or old wikitext that someone didn't want to actually delete.\n",
    "    \"\"\"\n",
    "    return re.sub('<!--.*?-->', '', wikitext, flags=re.DOTALL)\n",
    "\n",
    "spark.udf.register('removeComments', removeComments, 'String')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikitext Attributes\n",
    "These functions should all be called with code like:\n",
    "\n",
    "`WITH wikitext AS (\n",
    "    SELECT\n",
    "      revision_text\n",
    "    FROM wmf.mediawiki_wikitext_current\n",
    "    WHERE\n",
    "      snapshot = '{mediawiki_snapshot}'\n",
    "      AND ...\n",
    ")\n",
    "SELECT\n",
    "  LENGTH(getLinks(revision_text)) AS num_links,\n",
    "  getNumRefs(revision_text) AS num_refereneces,\n",
    "  ...\n",
    "FROM wikitext`\n",
    "\n",
    "NOTE: for most of these functions, there is generally a `mwparserfromhell` version and regex version. The regex versions are pretty good replacements and in theory faster / easier to deploy to workers because of their simplicity but do not catch as many edge-cases as `mwparserfromhell` does so if you have the luxury of using `mwparserfromhell`, go with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getLinksMWP(wikitext)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLinksMWP(wikitext):\n",
    "    \"\"\"Extract list of links from wikitext for an article via mwparserfromhell.\n",
    "    \n",
    "    This data is much more easily extracted via the pagelinks table but this wikitext-based approach\n",
    "    is important if you're trying to filter out links added via templates or need context about where\n",
    "    links are found on a page.\n",
    "    \n",
    "    NOTE: this approache removes the section anchors from links so it can be more easily matched against\n",
    "    other data about page titles but you could remove the `.partition('#')[0]` part to retain them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        return [str(l.title).partition('#')[0].replace(' ', '_').strip() for l in wt.filter_wikilinks()]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def getLinksRegex(wikitext):\n",
    "    \"\"\"Extract list of links from wikitext for an article via simple regex.\n",
    "    \n",
    "    Known Issues:\n",
    "    * Doesn't handle nested wikilinks (just gets the first) -- e.g., [[File:filename|caption=[[second_wikilink]]]] would be just File:filename\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [m.split('|')[0].partition('#')[0].replace(' ', '_').strip() for m in re.findall('(?<=\\[\\[)(.*?)(?=\\]\\])', wikitext, flags=re.DOTALL)]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    \n",
    "spark.udf.register('getLinks', getLinksMWP, 'Array<String>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getRefsMWP(wikitext)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfn_templates = [\"Shortened footnote template\", \"sfn\", \"Sfnp\", \"Sfnm\", \"Sfnmp\"]\n",
    "\n",
    "def getNumRefsMWP(wikitext):\n",
    "    \"\"\"Extract a count of references from wikitext for an article via mwparserfromhell.\n",
    "    \n",
    "    NOTE: the shortened footnote templates are used in some wikis but <ref> tags are definitely the standard\n",
    "    and it would be fair to just remove that portion of the code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        num_ref_tags = len([t.tag for t in wt.filter_tags() if t.tag == 'ref'])\n",
    "        num_sftn_templates = len([t.name for t in mwparserfromhell.parse(wikitext).filter_templates() if t.name in sfn_templates])\n",
    "        return num_ref_tags + num_sftn_templates\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def getNumRefsRegex(wikitext):\n",
    "    \"\"\"Regex version for extracting references.\n",
    "    \n",
    "    Not implemented here but see for potential code:\n",
    "    https://github.com/mediawiki-utilities/python-mwrefs/blob/master/mwrefs/references/extract.py\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "spark.udf.register('getNumRefs', getNumRefsMWP, 'Int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getHeadingsMWP(wikitext, level=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getHeadingsMWP(wikitext, level=None):\n",
    "    \"\"\"Extract list of headings from wikitext for an article via mwparserfromhell.\"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        if level is None:\n",
    "            return [str(l.title).strip() for l in wt.filter_headings()]\n",
    "        else:\n",
    "            return [str(l.title).strip() for l in wt.filter_headings() if l.level == level]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def getHeadingsRegex(wikitext, level=None):\n",
    "    \"\"\"Extract list of headings from wikitext for an article via simple regex.\"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        if level is None:\n",
    "            return [l[1].strip() for l in re.findall('(={2,})(.*?)(={2,})', wikitext)]\n",
    "        else:\n",
    "            return [l[1].strip() for l in re.findall('(={2,})(.*?)(={2,})', wikitext) if len(l[0]) == level]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getHeadings', getHeadingsMWP, 'Array<String>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getTemplatesMWP(wikitext)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTemplatesMWP(wikitext):\n",
    "    \"\"\"Extract list of templates from wikitext for an article via mwparserfromhell.\"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        return [str(t.name).replace(' ', '_').strip() for t in wt.filter_templates()]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def getTemplatesRegex(wikitext):\n",
    "    \"\"\"Extract list of templates from wikitext for an article via simple regex.\n",
    "\n",
    "    Known Issues:\n",
    "    * Doesn't handle nested templates (just gets the first)\n",
    "    -- e.g., '{{cite web|url=http://www.allmusic.com/|ref={{harvid|AllMusic}}}}' would be just web\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [m.split('|')[0].replace(' ', '_').strip() for m in re.findall('(?<=\\{\\{)(.*?)(?=\\}\\})', wikitext, flags=re.DOTALL)]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getTemplates', getTemplatesMWP, 'Array<String>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediawiki Table Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagelinks\n",
    "This query is targeted at Wikipedia and resolves redirects, maps page titles to page IDs and Wikidata IDs. It could be easily adapted to other namespaces, wikis, etc.\n",
    "\n",
    "CTEs:\n",
    "* `wikipedia_projects`: get wiki_dbs for all Wikipedia language editions -- e.g., `enwiki` for English Wikipedia\n",
    "* `title_to_id`: mapping of page ID to page title for each wiki\n",
    "* `redirects`: table of redirects in source page ID -> target page ID format\n",
    "* `pagelinks_reformatted`: pagelinks in source page ID -> target page ID format and with redirect pages removed (when they're the source of the link).\n",
    "* `pagelinks_redirects_resolved`: when target page ID is a redirect, resolve it. The `DISTINCT` clause ensures that no link pair occurs more than once (can happen when resolving redirects)\n",
    "* `wikidata_ids`: build table of page ID -> wikidata ID\n",
    "* `SELECT...`: join in Wikidata IDs to page links\n",
    "\n",
    "From this table, it would be easy to compute outlinks or inlinks:\n",
    "* `outlinks`: `SELECT wiki_db, pid_from, count(pid_to) AS num_outlinks FROM <links-table> GROUP BY wiki_db, pid_from`\n",
    "* `inlinks`: `SELECT wiki_db, pid_to, count(pid_from) AS num_inlinks FROM <links-table> GROUP BY wiki_db, pid_to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH wikipedia_projects AS (\n",
    "        SELECT DISTINCT dbname\n",
    "          FROM wmf_raw.mediawiki_project_namespace_map\n",
    "         WHERE snapshot = '{mediawiki_snapshot}'\n",
    "               AND hostname LIKE '%wikipedia%'\n",
    "        ),\n",
    "title_to_id AS (\n",
    "    SELECT page_id,\n",
    "           page_title,\n",
    "           wiki_db\n",
    "      FROM wmf_raw.mediawiki_page mp\n",
    "     INNER JOIN wikipedia_projects wp\n",
    "           ON (mp.wiki_db = wp.dbname)\n",
    "     WHERE page_namespace = 0\n",
    "           AND snapshot = '{mediawiki_snapshot}'\n",
    "),\n",
    "redirects AS (\n",
    "    SELECT mr.rd_from AS rd_from,\n",
    "           tti.page_id AS rd_to,\n",
    "           mr.wiki_db AS wiki_db\n",
    "      FROM wmf_raw.mediawiki_redirect mr\n",
    "     INNER JOIN title_to_id tti\n",
    "           ON (mr.rd_title = tti.page_title\n",
    "               AND mr.wiki_db = tti.wiki_db)\n",
    "     WHERE mr.snapshot = '{mediawiki_snapshot}'\n",
    "           AND mr.rd_namespace = 0\n",
    "),\n",
    "pagelinks_reformatted AS (\n",
    "    SELECT pl.pl_from AS pl_from,\n",
    "           tti.page_id AS pl_to,\n",
    "           pl.wiki_db AS wiki_db\n",
    "      FROM wmf_raw.mediawiki_pagelinks pl\n",
    "     INNER JOIN title_to_id tti\n",
    "           ON (pl.pl_title = tti.page_title\n",
    "               AND pl.wiki_db = tti.wiki_db)\n",
    "      LEFT ANTI JOIN redirects r\n",
    "           ON (pl.pl_from = r.rd_from\n",
    "               AND pl.wiki_db = r.wiki_db)\n",
    "     WHERE snapshot = '{mediawiki_snapshot}'\n",
    "           AND pl_from_namespace = 0\n",
    "           AND pl_namespace = 0\n",
    "),\n",
    "pagelinks_redirects_resolved AS (\n",
    "    SELECT DISTINCT pl.pl_from AS pl_from,\n",
    "           COALESCE(r.rd_to, pl.pl_to) AS pl_to,\n",
    "           pl.wiki_db AS wiki_db\n",
    "      FROM pagelinks_reformatted pl\n",
    "      LEFT JOIN redirects r\n",
    "           ON (pl.pl_to = r.rd_from\n",
    "               AND pl.wiki_db = r.wiki_db)\n",
    "),\n",
    "wikidata_ids AS (\n",
    "    SELECT DISTINCT wiki_db,\n",
    "           page_id,\n",
    "           item_id\n",
    "      FROM wmf.wikidata_item_page_link wd\n",
    "     INNER JOIN wikipedia_projects p\n",
    "           ON (wd.wiki_db = p.dbname)\n",
    "     WHERE wd.snapshot = '{wikidata_snapshot}'\n",
    "           AND wd.page_namespace = 0\n",
    ")\n",
    "SELECT wf.item_id AS qid_from,\n",
    "       p.pl_from AS pid_from,\n",
    "       wt.item_id AS qid_to,\n",
    "       p.pl_to AS pid_to,\n",
    "       p.wiki_db as wiki_db\n",
    "  FROM pagelinks_redirects_resolved p\n",
    "  LEFT JOIN wikidata_ids wf\n",
    "       ON (p.pl_from = wf.page_id\n",
    "           AND p.wiki_db = wf.wiki_db)\n",
    "  LEFT JOIN wikidata_ids wt\n",
    "       ON (p.pl_to = wt.page_id\n",
    "           AND p.wiki_db = wt.wiki_db)\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category links\n",
    "This is untested but my quick attempt at porting the pagelinks code to categories:\n",
    "\n",
    "Potential issues:\n",
    "* Do article redirect pages ever have categories? Should they be excluded explicitly?\n",
    "\n",
    "CTEs:\n",
    "* `wikipedia_projects`: get wiki_dbs for all Wikipedia language editions -- e.g., `enwiki` for English Wikipedia\n",
    "* `category_title_to_id`: mapping of page ID to page title for each wiki for categories\n",
    "* `article_title_to_id`: same but for articles\n",
    "* `redirects`: table of redirects in source page ID -> target page ID format. This is for categories.\n",
    "* `categorylinks_reformatted`: categorylinks in article page ID -> target category page ID format. \n",
    "* `categorylinks_redirects_resolved`: when target category is a redirect, resolve it. The `DISTINCT` clause ensures that no link pair occurs more than once (though I think in theory isn't necessary...).\n",
    "* `wikidata_ids`: build table of page ID -> wikidata ID for articles and categories\n",
    "* `SELECT...`: join in Wikidata IDs to category links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH wikipedia_projects AS (\n",
    "        SELECT DISTINCT dbname\n",
    "          FROM wmf_raw.mediawiki_project_namespace_map\n",
    "         WHERE snapshot = '{mediawiki_snapshot}'\n",
    "               AND hostname LIKE '%wikipedia%'\n",
    "        ),\n",
    "category_title_to_id AS (\n",
    "    SELECT page_id,\n",
    "           page_title,\n",
    "           wiki_db\n",
    "      FROM wmf_raw.mediawiki_page mp\n",
    "     INNER JOIN wikipedia_projects wp\n",
    "           ON (mp.wiki_db = wp.dbname)\n",
    "     WHERE page_namespace = 14\n",
    "           AND snapshot = '{mediawiki_snapshot}'\n",
    "),\n",
    "article_title_to_id AS (\n",
    "    SELECT page_id,\n",
    "           page_title,\n",
    "           wiki_db\n",
    "      FROM wmf_raw.mediawiki_page mp\n",
    "     INNER JOIN wikipedia_projects wp\n",
    "           ON (mp.wiki_db = wp.dbname)\n",
    "     WHERE page_namespace = 0\n",
    "           AND snapshot = '{mediawiki_snapshot}'\n",
    "),\n",
    "redirects AS (\n",
    "    SELECT r.rd_from AS rd_from,\n",
    "           cat.page_id AS rd_to,\n",
    "           r.wiki_db AS wiki_db\n",
    "      FROM wmf_raw.mediawiki_redirect r\n",
    "     INNER JOIN category_title_to_id cat\n",
    "           ON (r.rd_title = cat.page_title\n",
    "               AND r.wiki_db = cat.wiki_db)\n",
    "     WHERE mr.snapshot = '{mediawiki_snapshot}'\n",
    "           AND mr.rd_namespace = 14\n",
    "),\n",
    "categorylinks_reformatted AS (\n",
    "    SELECT cl.cl_from AS art_from,\n",
    "           cat.page_id AS cat_to,\n",
    "           cl.wiki_db AS wiki_db\n",
    "      FROM wmf_raw.mediawiki_categorylinks cl\n",
    "     INNER JOIN article_title_to_id art\n",
    "           ON (cl.cl_from = art.page_id\n",
    "               AND cl.wiki_db = art.wiki_db)\n",
    "     INNER JOIN category_title_to_id cat\n",
    "           ON (cl.pl_title = cat.page_title\n",
    "               AND cl.wiki_db = cat.wiki_db)\n",
    "     WHERE snapshot = '{mediawiki_snapshot}'\n",
    "),\n",
    "categorylinks_redirects_resolved AS (\n",
    "    SELECT DISTINCT cl.art_from AS art_from,\n",
    "           COALESCE(r.rd_to, cl.cat_to) AS cat_to,\n",
    "           cl.wiki_db AS wiki_db\n",
    "      FROM categorylinks_reformatted cl\n",
    "      LEFT JOIN redirects r\n",
    "           ON (cl.pl_to = r.rd_from\n",
    "               AND cl.wiki_db = r.wiki_db)\n",
    "),\n",
    "wikidata_ids AS (\n",
    "    SELECT DISTINCT wiki_db,\n",
    "           page_id,\n",
    "           item_id\n",
    "      FROM wmf.wikidata_item_page_link wd\n",
    "     INNER JOIN wikipedia_projects p\n",
    "           ON (wd.wiki_db = p.dbname)\n",
    "     WHERE wd.snapshot = '{wikidata_snapshot}'\n",
    "           AND (wd.page_namespace = 14\n",
    "                OR wd.page_namespace = 0)\n",
    ")\n",
    "SELECT wf.item_id AS qid_from,\n",
    "       p.art_from AS art_from,\n",
    "       wt.item_id AS qid_to,\n",
    "       p.cat_to AS cat_to,\n",
    "       p.wiki_db as wiki_db\n",
    "  FROM categorylinks_redirects_resolved p\n",
    "  LEFT JOIN wikidata_ids wf\n",
    "       ON (p.art_from = wf.page_id\n",
    "           AND p.wiki_db = wf.wiki_db)\n",
    "  LEFT JOIN wikidata_ids wt\n",
    "       ON (p.cat_to = wt.page_id\n",
    "           AND p.wiki_db = wt.wiki_db)\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image links\n",
    "\n",
    "Probably verify with Miriam or Aiko as there might be things about images I'm unaware of -- e.g., are there image redirects that I should be accounting for?\n",
    "\n",
    "CTEs:\n",
    "* `wikipedia_projects`: get wiki_dbs for all Wikipedia language editions -- e.g., `enwiki` for English Wikipedia\n",
    "* `pages`: set of all Wikipedia articles for gathering image data\n",
    "* `SELECT...`: gather all imagelinks for all articles and count up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH wikipedia_projects AS (\n",
    "    SELECT DISTINCT\n",
    "      dbname\n",
    "    FROM wmf_raw.mediawiki_project_namespace_map\n",
    "    WHERE\n",
    "      snapshot = '{mediawiki_snapshot}'\n",
    "      AND hostname LIKE '%wikipedia%'\n",
    "),\n",
    "pages AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      page_id\n",
    "    FROM wmf_raw.mediawiki_page p\n",
    "    INNER JOIN wikipedia_projects wp\n",
    "      ON (p.wiki_db = wp.dbname)\n",
    "    WHERE\n",
    "      snapshot = '{mediawiki_snapshot}'\n",
    "      AND page_namespace = 0\n",
    "      AND NOT page_is_redirect\n",
    ")\n",
    "SELECT\n",
    "  i.wiki_db,\n",
    "  il_from AS page_id,\n",
    "  COUNT(DISTINCT(il_to)) AS num_images\n",
    "FROM wmf_raw.mediawiki_imagelinks i\n",
    "INNER JOIN pages p\n",
    "  ON (i.il_from = p.page_id\n",
    "      AND i.wiki_db = p.wiki_db)\n",
    "WHERE\n",
    "  snapshot = '{mediawiki_snapshot}'\n",
    "  AND il_from_namespace = 0\n",
    "GROUP BY\n",
    "  i.wiki_db,\n",
    "  il_from\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equity Features\n",
    "* Gender: https://github.com/geohci/miscellaneous-wikimedia/blob/master/wikidata-properties-spark/wikidata_gender_information.ipynb\n",
    "* Geography: https://github.com/geohci/wiki-region-groundtruth/blob/main/wiki-region-data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featured articles\n",
    "This is sorta messy in that each wiki tracks featured articles differently. Could design a category-based approach but I think Wikidata badges are the way to go for a single universal source. Probably should do some evaluation to see if it captures everything we expect though.\n",
    "\n",
    "CTEs:\n",
    "* `wikipedia_projects`: get wiki_dbs for all Wikipedia language editions -- e.g., `enwiki` for English Wikipedia\n",
    "* `relevant_qids`: set of all Wikidata items with Wikipedia articles\n",
    "* `exploded_sitelinks`: for all relevant Wikidata items, get all sitelinks -- one row per sitelink (could retain the Wikidata `item_id` too here if desired). Each sitelink will have an associated page title for the Wikidata item and any information about badges.\n",
    "* `all_badges`: further expand out badge information for any sitelinks that have them.\n",
    "* `relevant_badges`: narrow down to just badges that indicate high-quality articles (Good, Featured, or Recommended)\n",
    "* `relevant_pages`: get all Wikipedia articles\n",
    "* `SELECT...`: keep badges that are associated with Wikipedia articles and join in page IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Article: Q17437798\n",
    "# Featured Article: Q17437796\n",
    "# Recommended Article: Q17559452\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH wikipedia_projects AS (\n",
    "    SELECT DISTINCT dbname\n",
    "      FROM wmf_raw.mediawiki_project_namespace_map\n",
    "     WHERE snapshot = '{mediawiki_snapshot}'\n",
    "           AND hostname LIKE '%wikipedia%'\n",
    "),\n",
    "relevant_qids AS (\n",
    "    SELECT DISTINCT item_id\n",
    "      FROM wmf.wikidata_item_page_link wd\n",
    "     INNER JOIN wikipedia_projects wp\n",
    "           ON (wd.wiki_db = wp.dbname)\n",
    "     WHERE snapshot = '{wikidata_snapshot}'\n",
    "           AND page_namespace = 0\n",
    "),\n",
    "exploded_sitelinks AS (\n",
    "    SELECT explode(sitelinks) as sitelink\n",
    "      FROM wmf.wikidata_entity w\n",
    "     INNER JOIN relevant_qids q\n",
    "           ON (w.id = q.item_id)\n",
    "     WHERE w.snapshot = '{wikidata_snapshot}'\n",
    "),\n",
    "all_badges AS (\n",
    "    SELECT sitelink.site AS wiki_db,\n",
    "           sitelink.title AS page_title,\n",
    "           EXPLODE(sitelink.badges) AS badge\n",
    "     FROM exploded_sitelinks\n",
    "    WHERE size(sitelink.badges) > 0\n",
    "),\n",
    "relevant_badges AS (\n",
    "    SELECT wiki_db,\n",
    "           page_title\n",
    "      FROM all_badges\n",
    "     WHERE badge IN ('Q17437798', 'Q17437796', 'Q17559452')\n",
    "),\n",
    "relevant_pages AS (\n",
    "    SELECT wiki_db,\n",
    "           page_id,\n",
    "           page_title\n",
    "      FROM wmf_raw.mediawiki_page p\n",
    "     INNER JOIN wikipedia_projects wp\n",
    "           ON (p.wiki_db = wp.dbname)\n",
    "     WHERE snapshot = '{mediawiki_snapshot}'\n",
    "           AND page_namespace = 0\n",
    ")\n",
    "SELECT b.wiki_db,\n",
    "       b.page_title,\n",
    "       p.page_id\n",
    "  FROM relevant_badges b\n",
    " INNER JOIN relevant_pages p\n",
    "       ON (b.wiki_db = p.wiki_db\n",
    "           AND b.page_title = p.page_title)\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
