{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import re\n",
    "\n",
    "import mwparserfromhell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1008.eqiad.wmnet:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook (isaacj -- pagerank)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7d0e1fa090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Pyspark notebook (isaacj -- pagerank)')\n",
    "    .master('yarn')\n",
    "    .config(\n",
    "        'spark.driver.extraJavaOptions',\n",
    "        ' '.join('-D{}={}'.format(k, v) for k, v in {\n",
    "            'http.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'http.proxyPort': '8080',\n",
    "            'https.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'https.proxyPort': '8080',\n",
    "        }.items()))\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11')\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config('spark.dynamicAllocation.maxExecutors', 128)\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 512)\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = '2020-07'  # data will be current to this date -- e.g., 2020-07 means data is up to 30 June 2020 (at least)\n",
    "wiki = 'enwiki'  # wikidb you want to run pagerank for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getLinks(wikitext)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLinks(wikitext):\n",
    "    \"\"\"Extract list of links from wikitext for an article.\"\"\"\n",
    "    try:\n",
    "        wt = mwparserfromhell.parse(wikitext)\n",
    "        return [str(l.title).partition('#')[0].replace(' ', '_') for l in wt.filter_wikilinks()]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getLinks', getLinks, 'Array<String>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH title_to_id AS (\n",
      "    SELECT page_id,\n",
      "           page_title\n",
      "      FROM wmf_raw.mediawiki_page\n",
      "     WHERE snapshot = '2020-07'\n",
      "           AND wiki_db = 'enwiki'\n",
      "           AND page_namespace = 0\n",
      "),\n",
      "redirects AS (\n",
      "    SELECT mr.rd_from AS rd_from,\n",
      "           tti.page_id AS rd_to\n",
      "      FROM wmf_raw.mediawiki_redirect mr\n",
      "     INNER JOIN title_to_id tti\n",
      "           ON (mr.rd_title = tti.page_title)\n",
      "     WHERE mr.snapshot = '2020-07'\n",
      "           AND mr.wiki_db = 'enwiki'\n",
      "           AND mr.rd_namespace = 0\n",
      "),\n",
      "pageLinks AS (\n",
      "    SELECT wt.page_id AS pl_from,\n",
      "           explode(getLinks(wt.revision_text)) AS pl_title_to\n",
      "      FROM wmf.mediawiki_wikitext_current wt\n",
      "      LEFT ANTI JOIN redirects r\n",
      "           ON (wt.page_id = r.rd_from)\n",
      "     WHERE wt.snapshot = '2020-07'\n",
      "           AND wt.wiki_db = 'enwiki'\n",
      "           AND wt.page_namespace = 0\n",
      "),\n",
      "pagelinks_reformatted AS (\n",
      "    SELECT pl.pl_from AS pl_from,\n",
      "           tti.page_id AS pl_to\n",
      "      FROM pageLinks pl\n",
      "     INNER JOIN title_to_id tti\n",
      "           ON (pl.pl_title_to = tti.page_title)\n",
      ")\n",
      "    SELECT DISTINCT pl.pl_from AS src,\n",
      "           COALESCE(r.rd_to, pl.pl_to) AS dst\n",
      "      FROM pagelinks_reformatted pl\n",
      "      LEFT JOIN redirects r\n",
      "           ON (pl.pl_to = r.rd_from)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Explanation of CTEs:\n",
    "* title_to_id: mapping of page title to page ID, which is a more stable identifier\n",
    "* redirects: resolve redirects so the network is much denser (~6M nodes instead of ~11M nodes)\n",
    "* pagelinks: extract links from wikitext and explode so each row has one link\n",
    "* pagelinks_reformatted: map link page titles to link page IDs\n",
    "* final: resolve redirects and rename columns to match pagerank library expectations\n",
    "\"\"\"\n",
    "\n",
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH title_to_id AS (\n",
    "    SELECT page_id,\n",
    "           page_title\n",
    "      FROM wmf_raw.mediawiki_page\n",
    "     WHERE snapshot = '{0}'\n",
    "           AND wiki_db = '{1}'\n",
    "           AND page_namespace = 0\n",
    "),\n",
    "redirects AS (\n",
    "    SELECT mr.rd_from AS rd_from,\n",
    "           tti.page_id AS rd_to\n",
    "      FROM wmf_raw.mediawiki_redirect mr\n",
    "     INNER JOIN title_to_id tti\n",
    "           ON (mr.rd_title = tti.page_title)\n",
    "     WHERE mr.snapshot = '{0}'\n",
    "           AND mr.wiki_db = '{1}'\n",
    "           AND mr.rd_namespace = 0\n",
    "),\n",
    "pagelinks AS (\n",
    "    SELECT wt.page_id AS pl_from,\n",
    "           explode(getLinks(wt.revision_text)) AS pl_title_to\n",
    "      FROM wmf.mediawiki_wikitext_current wt\n",
    "      LEFT ANTI JOIN redirects r\n",
    "           ON (wt.page_id = r.rd_from)\n",
    "     WHERE wt.snapshot = '{0}'\n",
    "           AND wt.wiki_db = '{1}'\n",
    "           AND wt.page_namespace = 0\n",
    "),\n",
    "pagelinks_reformatted AS (\n",
    "    SELECT pl.pl_from AS pl_from,\n",
    "           tti.page_id AS pl_to\n",
    "      FROM pagelinks pl\n",
    "     INNER JOIN title_to_id tti\n",
    "           ON (pl.pl_title_to = tti.page_title)\n",
    ")\n",
    "    SELECT DISTINCT pl.pl_from AS src,\n",
    "           COALESCE(r.rd_to, pl.pl_to) AS dst\n",
    "      FROM pagelinks_reformatted pl\n",
    "      LEFT JOIN redirects r\n",
    "           ON (pl.pl_to = r.rd_from)\n",
    "\"\"\".format(snapshot, wiki)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    src_dst = spark.sql(query)\n",
    "    src_dst.createOrReplaceTempView(\"src_dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|     src| dst|\n",
      "+--------+----+\n",
      "|  532476|2510|\n",
      "| 7007430|3052|\n",
      "|11826583|3052|\n",
      "|  386140|3052|\n",
      "| 4370492|6466|\n",
      "|45534636|6466|\n",
      "|22307781|6466|\n",
      "|38939808|6466|\n",
      "|28583057|6466|\n",
      "| 9277635|6466|\n",
      "+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_dst.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|src     |dst     |\n",
      "+--------+--------+\n",
      "|21349232|76508   |\n",
      "|21349232|1037934 |\n",
      "|21349232|17450530|\n",
      "|21349232|76762   |\n",
      "|21349232|44361   |\n",
      "|21349232|2789889 |\n",
      "|21349232|19599929|\n",
      "|21349232|52710   |\n",
      "|21349232|45773   |\n",
      "|21349232|12304303|\n",
      "|21349232|17851278|\n",
      "|21349232|30864733|\n",
      "|21349232|12319285|\n",
      "|21349232|17766207|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql('SELECT * FROM src_dst WHERE src = 21349232').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH all_pageids AS (\n",
      "    SELECT DISTINCT(page_id)\n",
      "      FROM (\n",
      "        SELECT src as page_id\n",
      "          FROM src_dst\n",
      "         UNION ALL\n",
      "        SELECT dst as page_id\n",
      "          FROM src_dst\n",
      "          ) p\n",
      "),\n",
      "pageid_to_title AS (\n",
      "    SELECT page_id,\n",
      "           page_title\n",
      "      FROM wmf_raw.mediawiki_page mp\n",
      "     WHERE snapshot = '2020-07'\n",
      "           AND wiki_db = 'enwiki'\n",
      "           AND page_namespace = 0\n",
      ")\n",
      "SELECT p.page_id as id,\n",
      "       t.page_title as page_title\n",
      "  FROM all_pageids p\n",
      "  LEFT JOIN pageid_to_title t\n",
      "            ON (p.page_id = t.page_id)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH all_pageids AS (\n",
    "    SELECT DISTINCT(page_id)\n",
    "      FROM (\n",
    "        SELECT src as page_id\n",
    "          FROM src_dst\n",
    "         UNION ALL\n",
    "        SELECT dst as page_id\n",
    "          FROM src_dst\n",
    "          ) p\n",
    "),\n",
    "pageid_to_title AS (\n",
    "    SELECT page_id,\n",
    "           page_title\n",
    "      FROM wmf_raw.mediawiki_page mp\n",
    "     WHERE snapshot = '{0}'\n",
    "           AND wiki_db = '{1}'\n",
    "           AND page_namespace = 0\n",
    ")\n",
    "SELECT p.page_id as id,\n",
    "       t.page_title as page_title\n",
    "  FROM all_pageids p\n",
    "  LEFT JOIN pageid_to_title t\n",
    "            ON (p.page_id = t.page_id)\n",
    "\"\"\".format(snapshot, wiki)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    nodes = spark.sql(query)\n",
    "    nodes.createOrReplaceTempView(\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|          page_title|\n",
      "+-----+--------------------+\n",
      "| 1365|             Ammonia|\n",
      "| 1990|            August_5|\n",
      "| 2835|        Afghan_Hound|\n",
      "| 2851|Abraham_Joshua_He...|\n",
      "| 7312|          Chauvinism|\n",
      "| 9762|  Ecumenical_council|\n",
      "| 9890|   Electron_counting|\n",
      "|10696|Military_of_the_F...|\n",
      "|14392|            Howitzer|\n",
      "|15392| Imperial_Conference|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create graph object\n",
    "g = GraphFrame(nodes, src_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|      id|inDegree|\n",
      "+--------+--------+\n",
      "| 5659330|     299|\n",
      "|18915364|      45|\n",
      "|  262135|     193|\n",
      "|   37299|   11652|\n",
      "|21224559|     223|\n",
      "|  273285|   37410|\n",
      "|  770909|    2021|\n",
      "|30928502|      32|\n",
      "|35106871|    1509|\n",
      "| 5610601|       5|\n",
      "+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html#graphframes.GraphFrame.pageRank\n",
    "# Hyperparameters:\n",
    "#   - resetProbability (inverse of damping factor: https://en.wikipedia.org/wiki/PageRank#Damping_factor)\n",
    "#     - most sources suggest it should be 0.15\n",
    "#   - maxIter is set to 40 here as that is the parameter used in: https://www.aifb.kit.edu/images/e/e5/Wikipedia_pagerank1.pdf\n",
    "#     - you could also set the tolerance to 0.01 but I don't know how long that takes to converge for enwiki\n",
    "# This shouldn't take more than 20-30 minutes for English Wikipedia\n",
    "# There will be k jobs you can track at https://yarn.wikimedia.org/cluster/scheduler where k is the number of iterations\n",
    "pr = g.pageRank(resetProbability=0.15, maxIter=40)\n",
    "result = pr.vertices.sort('pagerank', ascending=False)\n",
    "result.createOrReplaceTempView('pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write pagerank results to TSV\n",
    "query = \"\"\"\n",
    "SELECT pr.id as page_id,\n",
    "       pr.pagerank as pagerank,\n",
    "       n.page_title as page_title\n",
    "  FROM pagerank pr\n",
    "  LEFT JOIN nodes n\n",
    "       ON (pr.id = n.id)\n",
    "\"\"\"\n",
    "results = spark.sql(query)\n",
    "# this will write to 512 bzipped TSVs -- they can be easily compiled into 1 via Python or just use .coalesce(1) here\n",
    "# to pull onto stat machines: stat100x$ hdfs dfs -copyToLocal /user/isaacj/pagerank-enwiki/part* .\n",
    "results.write.csv(path=\"/user/isaacj/pagerank-{0}\".format(wiki), compression=\"bzip2\", header=True, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal pagerank-enwiki/part* file_parts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 512 files processed.\n",
      "2 / 512 files processed.\n",
      "4 / 512 files processed.\n",
      "8 / 512 files processed.\n",
      "16 / 512 files processed.\n",
      "32 / 512 files processed.\n",
      "64 / 512 files processed.\n",
      "128 / 512 files processed.\n",
      "256 / 512 files processed.\n",
      "512 / 512 files processed.\n",
      "6134966 pages processed. 0 skipped.\n"
     ]
    }
   ],
   "source": [
    "file_parts_dir = './file_parts/'\n",
    "fns = [fn for fn in os.listdir(file_parts_dir) if fn.endswith('.csv.bz2')]\n",
    "history_combined = 'enwiki_pagerank_notemplates.tsv'\n",
    "print_every = 1\n",
    "history_length = {}\n",
    "skipped = 0\n",
    "processed = 0\n",
    "output_header = ['page_id', 'pagerank', 'page_title']\n",
    "with open(history_combined, 'w') as fout:\n",
    "    fout.write('\\t'.join(output_header) + '\\n')\n",
    "    for i, fn in enumerate(fns, start=1):\n",
    "        with bz2.open(os.path.join(file_parts_dir, fn), 'rt') as fin:\n",
    "            # the quote symbol \" is somehow a valid username character...\n",
    "            header = next(fin).strip().split('\\t')\n",
    "            assert header == output_header\n",
    "            for line_no, line_str in enumerate(fin, start=1):\n",
    "                line = line_str.strip().split('\\t')\n",
    "                assert len(line) == len(output_header)\n",
    "                pid = line[0]\n",
    "                pagerank = line[1]\n",
    "                page_title = line[2]\n",
    "                try:\n",
    "                    int(pid)\n",
    "                except ValueError:\n",
    "                    print(\"PID:\", line_str)\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    float(pagerank)\n",
    "                except ValueError:\n",
    "                    print(\"PR:\", line_str)\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                processed += 1\n",
    "                fout.write(line_str)\n",
    "        if i % print_every == 0:\n",
    "            print(\"{0} / {1} files processed.\".format(i, len(fns)))\n",
    "            print_every = print_every * 2\n",
    "print(\"{0} pages processed. {1} skipped.\".format(processed, skipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./enwiki_pagerank_notemplates.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page_title\n",
       "United_States                           12560.054117\n",
       "World_War_II                             6399.805946\n",
       "France                                   6016.280689\n",
       "The_New_York_Times                       5477.629028\n",
       "United_Kingdom                           5381.480503\n",
       "List_of_sovereign_states                 5326.901882\n",
       "Germany                                  5153.956441\n",
       "India                                    4560.663176\n",
       "New_York_City                            4386.221642\n",
       "London                                   3828.579171\n",
       "Catholic_Church                          3675.257020\n",
       "Russia                                   3648.675568\n",
       "Italy                                    3620.371527\n",
       "English_language                         3530.305263\n",
       "Australia                                3498.592060\n",
       "Canada                                   3478.763155\n",
       "World_War_I                              3444.544972\n",
       "Japan                                    3417.589380\n",
       "China                                    3403.474038\n",
       "England                                  3386.117541\n",
       "Animal                                   3195.503788\n",
       "Iran                                     3190.897511\n",
       "U.S._state                               2991.754893\n",
       "Spain                                    2714.402980\n",
       "California                               2589.255500\n",
       "National_Register_of_Historic_Places     2589.201678\n",
       "The_Guardian                             2562.495473\n",
       "Arthropod                                2539.303384\n",
       "Paris                                    2535.296726\n",
       "Washington,_D.C.                         2518.926848\n",
       "Poland                                   2505.722127\n",
       "Soviet_Union                             2494.379980\n",
       "Netherlands                              2379.392908\n",
       "Insect                                   2320.103525\n",
       "Latin                                    2293.163811\n",
       "Brazil                                   2245.473455\n",
       "Association_football                     2189.517788\n",
       "Sweden                                   2105.222211\n",
       "Europe                                   2067.157925\n",
       "Mexico                                   2029.553176\n",
       "New_York_(state)                         1964.308588\n",
       "Switzerland                              1915.102776\n",
       "BBC                                      1848.179984\n",
       "United_States_Census_Bureau              1840.958445\n",
       "Los_Angeles                              1822.474632\n",
       "Scotland                                 1816.208373\n",
       "French_language                          1814.034067\n",
       "New_Zealand                              1767.012887\n",
       "Democratic_Party_(United_States)         1740.370811\n",
       "Norway                                   1739.840621\n",
       "Name: pagerank, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('pagerank', ascending=False).head(50).set_index('page_title')['pagerank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
