{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "!pip install git+https://github.com/wikimedia/wmfdata-python.git@release\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import wmfdata\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from graphframes import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1004.eqiad.wmnet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook (isaacj -- pagerank)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f04116875f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark_config = {\n",
    "    ## this configuration adds graphframes\n",
    "    'spark.driver.extraJavaOptions':\n",
    "        ' '.join('-D{}={}'.format(k, v) for k, v in {\n",
    "            'http.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'http.proxyPort': '8080',\n",
    "            'https.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'https.proxyPort': '8080',\n",
    "        }.items()),\n",
    "    'spark.jars.packages':'graphframes:graphframes:0.6.0-spark2.3-s_2.11'\n",
    "}\n",
    "\n",
    "# Make sure to update with your username\n",
    "# Easy to find in https://yarn.wikimedia.org/cluster/scheduler for tracking progress then\n",
    "spark = wmfspark.get_session(\n",
    "    type='large',\n",
    "    app_name='Pyspark notebook (<username> -- pagerank)',\n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Outlinks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = '2020-07'  # data will be current to this date -- e.g., 2020-07 means data is up to 30 June 2020 (at least)\n",
    "wiki = 'enwiki'  # wikidb you want to run pagerank for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather edges dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH title_to_id AS (\n",
      "    SELECT page_id,\n",
      "           page_title,\n",
      "           wiki_db\n",
      "      FROM wmf_raw.mediawiki_page mp\n",
      "     WHERE page_namespace = 0\n",
      "           AND snapshot = '2020-07'\n",
      "           AND wiki_db = 'enwiki'\n",
      "),\n",
      "redirects AS (\n",
      "    SELECT mr.rd_from AS rd_from,\n",
      "           tti.page_id AS rd_to,\n",
      "           mr.wiki_db AS wiki_db\n",
      "      FROM wmf_raw.mediawiki_redirect mr\n",
      "     INNER JOIN title_to_id tti\n",
      "           ON (mr.rd_title = tti.page_title\n",
      "               AND mr.wiki_db = tti.wiki_db)\n",
      "     WHERE mr.snapshot = '2020-07'\n",
      "           AND mr.rd_namespace = 0\n",
      "           AND mr.wiki_db = 'enwiki'\n",
      "),\n",
      "pagelinks_reformatted AS (\n",
      "    SELECT pl.pl_from AS pl_from,\n",
      "           tti.page_id AS pl_to,\n",
      "           pl.wiki_db AS wiki_db\n",
      "      FROM wmf_raw.mediawiki_pagelinks pl\n",
      "     INNER JOIN title_to_id tti\n",
      "           ON (pl.pl_title = tti.page_title\n",
      "               AND pl.wiki_db = tti.wiki_db)\n",
      "      LEFT ANTI JOIN redirects r\n",
      "           ON (pl.pl_from = r.rd_from\n",
      "               AND pl.wiki_db = r.wiki_db)\n",
      "     WHERE snapshot = '2020-07'\n",
      "           AND pl_from_namespace = 0\n",
      "           AND pl_namespace = 0\n",
      "           AND pl.wiki_db = 'enwiki'\n",
      ")\n",
      "    SELECT DISTINCT pl.pl_from AS src,\n",
      "           COALESCE(r.rd_to, pl.pl_to) AS dst\n",
      "      FROM pagelinks_reformatted pl\n",
      "      LEFT JOIN redirects r\n",
      "           ON (pl.pl_to = r.rd_from\n",
      "               AND pl.wiki_db = r.wiki_db)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH title_to_id AS (\n",
    "    SELECT page_id,\n",
    "           page_title,\n",
    "           wiki_db\n",
    "      FROM wmf_raw.mediawiki_page mp\n",
    "     WHERE page_namespace = 0\n",
    "           AND snapshot = '{0}'\n",
    "           AND wiki_db = '{1}'\n",
    "),\n",
    "redirects AS (\n",
    "    SELECT mr.rd_from AS rd_from,\n",
    "           tti.page_id AS rd_to,\n",
    "           mr.wiki_db AS wiki_db\n",
    "      FROM wmf_raw.mediawiki_redirect mr\n",
    "     INNER JOIN title_to_id tti\n",
    "           ON (mr.rd_title = tti.page_title\n",
    "               AND mr.wiki_db = tti.wiki_db)\n",
    "     WHERE mr.snapshot = '{0}'\n",
    "           AND mr.rd_namespace = 0\n",
    "           AND mr.wiki_db = '{1}'\n",
    "),\n",
    "pagelinks_reformatted AS (\n",
    "    SELECT pl.pl_from AS pl_from,\n",
    "           tti.page_id AS pl_to,\n",
    "           pl.wiki_db AS wiki_db\n",
    "      FROM wmf_raw.mediawiki_pagelinks pl\n",
    "     INNER JOIN title_to_id tti\n",
    "           ON (pl.pl_title = tti.page_title\n",
    "               AND pl.wiki_db = tti.wiki_db)\n",
    "      LEFT ANTI JOIN redirects r\n",
    "           ON (pl.pl_from = r.rd_from\n",
    "               AND pl.wiki_db = r.wiki_db)\n",
    "     WHERE snapshot = '{0}'\n",
    "           AND pl_from_namespace = 0\n",
    "           AND pl_namespace = 0\n",
    "           AND pl.wiki_db = '{1}'\n",
    ")\n",
    "    SELECT DISTINCT pl.pl_from AS src,\n",
    "           COALESCE(r.rd_to, pl.pl_to) AS dst\n",
    "      FROM pagelinks_reformatted pl\n",
    "      LEFT JOIN redirects r\n",
    "           ON (pl.pl_to = r.rd_from\n",
    "               AND pl.wiki_db = r.wiki_db)\n",
    "\"\"\".format(snapshot, wiki)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    src_dst = spark.sql(query)\n",
    "    src_dst.createOrReplaceTempView(\"src_dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|     src| dst|\n",
      "+--------+----+\n",
      "|17178107|3149|\n",
      "|59811191|4348|\n",
      "|30270910|4348|\n",
      "|11117831|4348|\n",
      "|48534267|4348|\n",
      "| 5795377|4348|\n",
      "| 7928030|4348|\n",
      "| 5765223|4348|\n",
      "|42045679|4348|\n",
      "| 6165691|9334|\n",
      "+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_dst.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather node metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH all_pageids AS (\n",
      "    SELECT DISTINCT(page_id)\n",
      "      FROM (\n",
      "        SELECT src as page_id\n",
      "          FROM src_dst\n",
      "         UNION ALL\n",
      "        SELECT dst as page_id\n",
      "          FROM src_dst\n",
      "          ) p\n",
      "),\n",
      "pageid_to_title AS (\n",
      "    SELECT page_id,\n",
      "           page_title,\n",
      "           wiki_db\n",
      "      FROM wmf_raw.mediawiki_page mp\n",
      "     WHERE page_namespace = 0\n",
      "           AND snapshot = '2020-07'\n",
      "           AND wiki_db = 'enwiki'\n",
      ")\n",
      "SELECT p.page_id as id,\n",
      "       t.page_title as page_title\n",
      "  FROM all_pageids p\n",
      "  LEFT JOIN pageid_to_title t\n",
      "            ON (p.page_id = t.page_id)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH all_pageids AS (\n",
    "    SELECT DISTINCT(page_id)\n",
    "      FROM (\n",
    "        SELECT src as page_id\n",
    "          FROM src_dst\n",
    "         UNION ALL\n",
    "        SELECT dst as page_id\n",
    "          FROM src_dst\n",
    "          ) p\n",
    "),\n",
    "pageid_to_title AS (\n",
    "    SELECT page_id,\n",
    "           page_title,\n",
    "           wiki_db\n",
    "      FROM wmf_raw.mediawiki_page mp\n",
    "     WHERE page_namespace = 0\n",
    "           AND snapshot = '{0}'\n",
    "           AND wiki_db = '{1}'\n",
    ")\n",
    "SELECT p.page_id as id,\n",
    "       t.page_title as page_title\n",
    "  FROM all_pageids p\n",
    "  LEFT JOIN pageid_to_title t\n",
    "            ON (p.page_id = t.page_id)\n",
    "\"\"\".format(snapshot, wiki)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    nodes = spark.sql(query)\n",
    "    nodes.createOrReplaceTempView(\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|          page_title|\n",
      "+-----+--------------------+\n",
      "| 1365|             Ammonia|\n",
      "| 1990|            August_5|\n",
      "| 2835|        Afghan_Hound|\n",
      "| 2851|Abraham_Joshua_He...|\n",
      "| 7312|          Chauvinism|\n",
      "| 9762|  Ecumenical_council|\n",
      "| 9890|   Electron_counting|\n",
      "|10696|Military_of_the_F...|\n",
      "|14392|            Howitzer|\n",
      "|15392| Imperial_Conference|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create graph object\n",
    "g = GraphFrame(nodes, src_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|      id|inDegree|\n",
      "+--------+--------+\n",
      "|14625636|      72|\n",
      "| 6815074|      95|\n",
      "|   37299|   12926|\n",
      "|  912025|    1900|\n",
      "|34446095|     493|\n",
      "|54645354|       1|\n",
      "|  614284|     665|\n",
      "|63426542|      31|\n",
      "|  109900|      38|\n",
      "|21830778|     320|\n",
      "+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html#graphframes.GraphFrame.pageRank\n",
    "# Hyperparameters:\n",
    "#   - resetProbability (inverse of damping factor: https://en.wikipedia.org/wiki/PageRank#Damping_factor)\n",
    "#     - most sources suggest it should be 0.15\n",
    "#   - maxIter is set to 40 here as that is the parameter used in: https://www.aifb.kit.edu/images/e/e5/Wikipedia_pagerank1.pdf\n",
    "#     - you could also set the tolerance to 0.01 but I don't know how long that takes to converge for enwiki\n",
    "# This shouldn't take more than 20-30 minutes for English Wikipedia\n",
    "# There will be k jobs you can track at https://yarn.wikimedia.org/cluster/scheduler where k is the number of iterations\n",
    "pr = g.pageRank(resetProbability=0.15, maxIter=40)\n",
    "result = pr.vertices.sort('pagerank', ascending=False)\n",
    "result.createOrReplaceTempView('pagerank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write pagerank results to TSV\n",
    "query = \"\"\"\n",
    "SELECT pr.id as page_id,\n",
    "       pr.pagerank as pagerank,\n",
    "       n.page_title as page_title\n",
    "  FROM pagerank pr\n",
    "  LEFT JOIN nodes n\n",
    "       ON (pr.id = n.id)\n",
    "\"\"\"\n",
    "results = spark.sql(query)\n",
    "# this will write to 512 bzipped TSVs -- they can be easily compiled into 1 via Python or just use .coalesce(1) here\n",
    "# to pull onto stat machines: stat100x$ hdfs dfs -copyToLocal /user/isaacj/pagerank-enwiki/part* .\n",
    "results.write.csv(path=\"/user/isaacj/pagerank-{0}\".format(wiki), compression=\"bzip2\", header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
