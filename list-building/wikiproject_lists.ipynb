{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_mariadb_stat2(query, db, filename=None, verbose=True):\n",
    "    \"\"\"Query MariaDB.\"\"\"\n",
    "    if db in DB_METADATA:\n",
    "        node = DB_METADATA[db]['node']\n",
    "    else:\n",
    "        raise NotImplementedError(\"Don't know mapping of db {0} to mysql node.\".format(db))\n",
    "    cmd = ('mysql --defaults-extra-file=/etc/mysql/conf.d/analytics-research-client.cnf '\n",
    "           '-h s{0}-analytics-replica.eqiad.wmnet -P 331{0} -A --database {1} -e \"{2}\"'.format(node, db, query))\n",
    "    if filename:\n",
    "        cmd = cmd + \" > \" + filename\n",
    "    if verbose:\n",
    "        print(' '.join(cmd.split()))\n",
    "    ret = os.system(cmd)\n",
    "    return ret\n",
    "\n",
    "def exec_hive_stat2(query, filename=None, priority=False, verbose=True, nice=False, large=False):\n",
    "    \"\"\"Query Hive.\"\"\"\n",
    "    if priority:\n",
    "        query = \"SET mapreduce.job.queuename=priority;\" + query\n",
    "    elif large:\n",
    "        query = \"SET mapreduce.job.queuename=nice; SET mapreduce.map.memory.mb=4096;\" + query # SET mapreduce.map.memory.mb=4096\n",
    "    elif nice:\n",
    "        query = \"SET mapreduce.job.queuename=nice;\" + query\n",
    "        # if issues: SET mapred.job.queue.name=nice;\n",
    "    cmd = \"\"\"hive -e \\\" \"\"\" + query + \"\"\" \\\"\"\"\"\n",
    "    if filename:\n",
    "        cmd = cmd + \" > \" + filename\n",
    "    if verbose:\n",
    "        print(' '.join(cmd.split()))\n",
    "    ret = os.system(cmd)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_wp_name_ar(wp):\n",
    "    ns_local = 'ويكيبيديا'\n",
    "    return re.sub(\"\\s\\s+\", \" \", wp.lower().replace(ns_local + \":\", \"\").replace('مشروع ويكي', '').strip())\n",
    "\n",
    "def norm_wp_name_en(wp):\n",
    "    ns_local = 'wikipedia'\n",
    "    wp_prefix = 'wikiproject'\n",
    "    return re.sub(\"\\s\\s+\", \" \", wp.lower().replace(ns_local + \":\", \"\").replace(wp_prefix, \"\").strip())\n",
    "\n",
    "def norm_wp_name_hu(wp):\n",
    "    ns_local = 'wikipédia'\n",
    "    to_strip = [ns_local + \":\", 'témájú', 'kapcsolatos', 'műhelyek', 'műhely', '-es ', '-', 'országgal', 'ország']\n",
    "    hardcoded_matches = {'Wikipédia:Harry Potter-műhely':'Harry Potterrel kapcsolatos',\n",
    "                         'Wikipédia:USA-műhely':'USA-val kapcsolatos',\n",
    "                         'Wikipédia:Anime- és mangaműhely':'anime-manga témájú',\n",
    "                         'Wikipédia:Első világháború műhely':'első világháborús témájú'}\n",
    "    for m in hardcoded_matches:\n",
    "        if wp == m:\n",
    "            wp = hardcoded_matches[m]\n",
    "    wp = wp.lower()\n",
    "    for s in to_strip:\n",
    "        wp = wp.replace(s, ' ')\n",
    "    return re.sub(\"\\s\\s+\", \" \", wp.strip())\n",
    "\n",
    "def norm_wp_name_fr(wp):\n",
    "    ns_local = 'projet'\n",
    "    return re.sub(\"\\s\\s+\", \" \", wp.lower().replace(ns_local + ':', \"\").strip())\n",
    "\n",
    "def norm_wp_name_tr(wp):\n",
    "    ns_local = 'vikiproje'\n",
    "    wp_prefix = 'vikipedi'\n",
    "    return re.sub(\"\\s\\s+\", \" \", wp.lower().replace(wp_prefix, \"\").replace(ns_local, '').replace(':', '').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Groundtruth Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DB_METADATA = {'enwiki':{'node':1, 'norm':norm_wp_name_en},  # 21M pages\n",
    "               'frwiki':{'node':6, 'norm':norm_wp_name_fr},  #  2.7M pages\n",
    "               'arwiki':{'node':7, 'norm':norm_wp_name_ar},  #  2.8M pages\n",
    "               'huwiki':{'node':7, 'norm':norm_wp_name_hu},  #    330K pages\n",
    "               'trwiki':{'node':2, 'norm':norm_wp_name_tr}   #    280K pages\n",
    "               }\n",
    "\n",
    "STANDARDIZE = {'top': 'Top',\n",
    "               'Top': 'Top',\n",
    "               'High': 'High',\n",
    "               'high': 'High',\n",
    "               'mid': 'Mid',\n",
    "               'Mid': 'Mid',\n",
    "               'Related': 'Low',\n",
    "               'Bottom': 'Low',\n",
    "               'low': 'Low',\n",
    "               'Low': 'Low'}\n",
    "IMP_PLACEHOLDER = None\n",
    "IMP_RANKING = {'Top':1, 'High':2, 'Mid':3, 'Low':4, None:5}\n",
    "\n",
    "page_assessments_tsv = './wikiproject_data.tsv'\n",
    "db = 'enwiki'\n",
    "dbname_snapshot = '2020-12'\n",
    "pid_to_qid_snapshot = '2021-01-04'\n",
    "pid_to_qid_tsv = './groundtruth/pid_to_qid.tsv'\n",
    "output_tsv = './list_building_en_groundtruth.tsv.bz2'\n",
    "norm_fn = DB_METADATA[db]['norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mapping of pageID to list of all associated WikiProjects via page_assessments table in MariaDB\n",
    "if not os.path.exists(page_assessments_tsv):\n",
    "    print(\"Gathering page assessments data and writing to:\", page_assessments_tsv)\n",
    "    start_time = time.time()\n",
    "    query = \"\"\"\n",
    "    SELECT pa.pa_page_id AS article_pid,\n",
    "           p.page_title AS title,\n",
    "           pap.pap_project_title AS wp_template,\n",
    "           pa.pa_importance AS importance\n",
    "      FROM page_assessments pa\n",
    "     INNER JOIN page_assessments_projects pap\n",
    "           ON (pa.pa_project_id = pap.pap_project_id)\n",
    "     INNER JOIN page p\n",
    "           ON (pa.pa_page_id = p.page_id AND p.page_namespace = 0 and p.page_is_redirect = 0)\n",
    "     ORDER BY pap.pap_project_title ASC\n",
    "    \"\"\"\n",
    "    exec_mariadb_stat2(query=query, db=db, filename=page_assessments_tsv, verbose=True)\n",
    "    print(\"Page assessments complete after {0:.1f} minutes!\".format((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for QIDs / sitelinks\n",
    "if not os.path.exists(pid_to_qid_tsv):\n",
    "    print(\"Gathering PID / QID mapping and writing to:\", pid_to_qid_tsv)\n",
    "    start_time = time.time()\n",
    "    query = f\"\"\"\n",
    "    WITH wikipedia_projects AS (\n",
    "        SELECT DISTINCT\n",
    "          dbname\n",
    "        FROM wmf_raw.mediawiki_project_namespace_map\n",
    "        WHERE\n",
    "          snapshot = '{dbname_snapshot}'\n",
    "          AND hostname = 'en.wikipedia'\n",
    "    )\n",
    "    SELECT\n",
    "      item_id,\n",
    "      page_id,\n",
    "      wiki_db\n",
    "    FROM wmf.wikidata_item_page_link wd\n",
    "    INNER JOIN wikipedia_projects wp\n",
    "      ON (wd.wiki_db = wp.dbname)\n",
    "    WHERE\n",
    "      snapshot = '{pid_to_qid_snapshot}'\n",
    "      AND page_namespace = 0\n",
    "    \"\"\"\n",
    "    exec_hive_stat2(query, filename=pid_to_qid_tsv, priority=False, verbose=True, nice=True, large=False)\n",
    "    print(\"PID / QID mapping complete after {0:.1f} minutes!\".format((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6493524 pages in enwiki with Wikidata IDs\n"
     ]
    }
   ],
   "source": [
    "pid_to_qid = {}\n",
    "with open(pid_to_qid_tsv, 'r') as fin:\n",
    "    tsvreader = csv.reader(fin, delimiter='\\t')\n",
    "    assert next(tsvreader) == ['item_id', 'page_id', 'wiki_db']\n",
    "    for line in tsvreader:\n",
    "        wiki_db = line[2]\n",
    "        if wiki_db == db:\n",
    "            qid = line[0]\n",
    "            pid = int(line[1])\n",
    "            pid_to_qid[pid] = qid\n",
    "print(\"{0} pages in {1} with Wikidata IDs\".format(len(pid_to_qid), db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_pid\ttitle\twp_template\timportance\r\n",
      "29305\tSojourner_Truth\t1000 Women in Religion\t\r\n",
      "391183\tFrances_Harper\t1000 Women in Religion\t\r\n",
      "478677\tEadburh_of_Bicester\t1000 Women in Religion\t\r\n",
      "1607259\tVida_Goldstein\t1000 Women in Religion\t\r\n",
      "1803216\tAldobrandesca\t1000 Women in Religion\t\r\n",
      "3379384\tColumba_of_Cornwall\t1000 Women in Religion\t\r\n",
      "3388407\tColumba_of_Sens\t1000 Women in Religion\t\r\n",
      "3397671\tColumba_of_Spain\t1000 Women in Religion\t\r\n",
      "4566276\tÁurea_of_San_Millán\t1000 Women in Religion\t\r\n"
     ]
    }
   ],
   "source": [
    "!head {page_assessments_tsv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('+inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_order(output_row):\n",
    "    importance = output_row[4]\n",
    "    imp_rank = IMP_RANKING[importance]\n",
    "    qid = output_row[0]\n",
    "    if qid is None:\n",
    "        qid = 'Q+inf'\n",
    "    qid_rank = float(qid[1:])\n",
    "    pid = output_row[1]\n",
    "    return (imp_rank, qid_rank, pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_header = ['article_pid', 'title', 'wp_template', 'importance']\n",
    "nonstandard = {}\n",
    "pids_matched = 0\n",
    "written = 0\n",
    "with bz2.open(output_tsv, 'wt') as fout:\n",
    "    tsvwriter = csv.writer(fout, delimiter='\\t')\n",
    "    tsvwriter.writerow(['article_qid'] + expected_header)\n",
    "    with open(page_assessments_tsv, 'r') as fin:\n",
    "        tsvreader = csv.reader(fin, delimiter='\\t')\n",
    "        assert next(tsvreader) == expected_header\n",
    "        current_wp = None\n",
    "        data = []\n",
    "        for line in tsvreader:\n",
    "            pid = int(line[0])\n",
    "            title = line[1]\n",
    "            wp_template = line[2]\n",
    "            importance = line[3]\n",
    "            if importance in STANDARDIZE:\n",
    "                importance = STANDARDIZE[importance]\n",
    "            else:\n",
    "                nonstandard[importance] = nonstandard.get(importance, 0) + 1\n",
    "                importance = IMP_PLACEHOLDER\n",
    "            qid = pid_to_qid.get(pid, None)\n",
    "            if qid:\n",
    "                pids_matched += 1\n",
    "                \n",
    "            if wp_template != current_wp:\n",
    "                if data:\n",
    "                    data = sorted(data, key=lambda x: sort_order(x))\n",
    "                    for line in data:\n",
    "                        tsvwriter.writerow(line)\n",
    "                        written += 1\n",
    "                data = []\n",
    "                current_wp = wp_template\n",
    "\n",
    "            data.append([qid, pid, title, wp_template, importance])\n",
    "\n",
    "        if data:\n",
    "            data = sorted(data, key=lambda x: sort_order(x))\n",
    "            for line in data:\n",
    "                tsvwriter.writerow(line)\n",
    "                written += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16364421 written. 16252875 had QIDs.\n",
      "\n",
      "Non-standard importance classes:\n",
      " 4908597\n",
      "Unknown 2886755\n",
      "NA 113221\n",
      "{{<span class=\"error 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{written} written. {pids_matched} had QIDs.\")\n",
    "print(\"\\nNon-standard importance classes:\")\n",
    "for l in sorted(nonstandard, key=nonstandard.get, reverse=True):\n",
    "    print(l, nonstandard[l])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
